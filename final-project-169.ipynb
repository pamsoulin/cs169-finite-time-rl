{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code For: Experimentally Comparing Reinforcement Learning Algorithms with Finite-Time Optimality Guarantees\n",
    "\n",
    "Contributors: Andrew Lee, Nathan Ng, Sam Poulin, Rose Zhang, and Fivos Kalogiannis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the Reinforcement Learning algorithms, we first implement a version of Gridworld to test the algorithms on and define constants in.\n",
    "\n",
    "We first begin with the necessary inputs for the rest of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Arrow, Circle\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a class that allows us to store and represent a Grid World, alongside some requisite parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    def __init__(self, w, h, gamma, rho = None):\n",
    "        self.RX = w\n",
    "        self.RY = h\n",
    "        self.NSTATES = w*h\n",
    "        self.NACTIONS = 5\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if rho is None:\n",
    "            self.rho = np.zeros(self.NSTATES)\n",
    "            self.rho[0] = 1.0\n",
    "        else:\n",
    "            self.rho = rho\n",
    "\n",
    "        #setup transition matrix:\n",
    "        self.P = np.zeros( (self.RY, self.RX, self.NACTIONS, self.RY, self.RX) )\n",
    "        for idx in range(self.RY*self.RX):\n",
    "            x = idx % self.RX\n",
    "            y = idx // self.RX\n",
    "            for a in range(self.NACTIONS):\n",
    "                ny, nx = self.move(y, x, a)\n",
    "                ny = np.clip(ny, 0, self.RY-1)\n",
    "                nx = np.clip(nx, 0, self.RX-1)\n",
    "                self.P[y, x, a, ny, nx] = 1\n",
    "\n",
    "        R = np.zeros((self.RY, self.RX, self.NACTIONS))\n",
    "        self.R = R.reshape(self.RY*self.RX, self.NACTIONS)\n",
    "    \n",
    "    def set_reward(self, x, y, val):\n",
    "        self.R = self.R.reshape(self.RY, self.RX, self.NACTIONS)\n",
    "        if x not in range(0, self.RX) or y not in range(0, self.RY):\n",
    "            return\n",
    "        self.R[y, x, 1] = val\n",
    "        if y-1 in range(0, self.RY):\n",
    "            self.R[y-1, x, 3] = val\n",
    "        if y+1 in range(0, self.RY): \n",
    "            self.R[y+1, x, 4] = val\n",
    "        if x-1 in range(0, self.RX): \n",
    "            self.R[y, x-1, 2] = val\n",
    "        if x+1 in range(0, self.RX):\n",
    "            self.R[y, x+1, 0] = val\n",
    "        self.R = self.R.reshape(self.RY*self.RX, self.NACTIONS)\n",
    "\n",
    "    def move(self, y, x, a):\n",
    "        if a == 0:\n",
    "            x -= 1\n",
    "            return y, x\n",
    "        elif a == 1:\n",
    "            return y, x\n",
    "        elif a == 2:\n",
    "            x += 1\n",
    "            return y, x\n",
    "        elif a == 3:\n",
    "            y += 1\n",
    "            return y, x\n",
    "        elif a == 4:\n",
    "            y -= 1\n",
    "            return y, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we are able to implement a Projected Gradient Ascent Reinforcement Learning algorithm to solve this implementation of Grid World.\n",
    "\n",
    "We begin by defining functions used when calculating the value and gradient of a given policy for each step of the Projected Gradient algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d(gamma, Ptheta):\n",
    "    sz = np.shape(Ptheta)\n",
    "    return np.linalg.inv(np.eye(*sz) - gamma * Ptheta)\n",
    "\n",
    "def get_V(gw, policy, d):\n",
    "    #print(np.shape(policy))\n",
    "    #print(np.shape(gw.R))\n",
    "    r_expect = np.sum(policy * gw.R, axis=1)\n",
    "    V = np.zeros(gw.NSTATES)\n",
    "    V = d.dot(r_expect)\n",
    "    #print(V.shape)\n",
    "    return V\n",
    "\n",
    "def get_Q(gw, policy, d):\n",
    "    V = get_V(gw, policy, d)\n",
    "    return gw.R + gw.gamma * np.einsum( 'ijk,k->ij', gw.P.reshape( gw.NSTATES, gw.NACTIONS, gw.NSTATES) , V )\n",
    "\n",
    "def get_Ptheta2D(P, policy, gw):\n",
    "    Pprime = P.reshape(gw.NSTATES * gw.NACTIONS, gw.NSTATES) * policy.reshape(gw.NSTATES * gw.NACTIONS)[:, None]\n",
    "    Ptheta = np.zeros((gw.NSTATES, gw.NSTATES))\n",
    "\n",
    "    for s in range(gw.NSTATES):\n",
    "        z = np.sum( Pprime[s*gw.NACTIONS:(s+1)*gw.NACTIONS, :], axis=0 )\n",
    "        Ptheta[s, :] = z\n",
    "    return Ptheta\n",
    "\n",
    "def get_direct_grad(gw, policy):\n",
    "    Ptheta = get_Ptheta2D(gw.P, policy, gw)\n",
    "    d = get_d(gw.gamma, Ptheta)\n",
    "    V = get_V(gw, policy, d)\n",
    "    q = get_Q(gw, policy, d)\n",
    "    grad = (1/(1 - gw.gamma)) * gw.rho.dot(d)[:, None] * q\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then implement an algorithm for projecting onto a simplex described by Yunmei Chen and Xiaojing Ye ([arXiv:1101.6081](https://arxiv.org/abs/1101.6081)) to ensure that each state in a policy describes a valid probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projsplx(y):\n",
    "    \"\"\"\n",
    "    Python implementation of:\n",
    "    https://arxiv.org/abs/1101.6081\n",
    "    \"\"\"\n",
    "    s = np.sort(y)\n",
    "    n = len(y) ; flag = False\n",
    "\n",
    "    parsum = 0\n",
    "    tmax = -np.inf\n",
    "    for idx in range(n-2, -1, -1):\n",
    "        parsum += s[idx+1]\n",
    "        tmax = (parsum - 1) / (n - (idx + 1) )\n",
    "        if tmax >= s[idx]:\n",
    "            flag = True ; break\n",
    "\n",
    "    if not flag:\n",
    "        tmax = (np.sum(s) - 1) / n\n",
    "\n",
    "    return np.maximum(y - tmax, 0)\n",
    "\n",
    "def policy_projsplx(policy):\n",
    "    new_pol = np.zeros_like(policy)\n",
    "    for idx, row in enumerate(policy):\n",
    "        new_pol[idx, :] = projsplx(row.flatten())\n",
    "    return new_pol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the functions defined above to implement a Projected Gradient Ascent Reinforcement Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_ascent(gw, T = 1000, eta = 0.3):\n",
    "    policy = np.random.rand(gw.RY, gw.RX, gw.NACTIONS)\n",
    "    policy = policy.reshape(gw.RY*gw.RX, gw.NACTIONS)\n",
    "    \n",
    "    #print(policy.shape)\n",
    "\n",
    "    for idx, row in enumerate(policy):\n",
    "        policy[idx, :] = projsplx(row.flatten())\n",
    "\n",
    "    Vs = []\n",
    "    for _ in range(T + 1):\n",
    "        ptheta = get_Ptheta2D(gw.P, policy, gw)\n",
    "        d = get_d(gw.gamma, ptheta)\n",
    "        V = get_V(gw, policy, d)\n",
    "        Vs.append(gw.rho.dot(V))\n",
    "        \n",
    "        grad = get_direct_grad(gw, policy)\n",
    "        policy = policy + eta * grad\n",
    "        for idx, row in enumerate(policy):\n",
    "            policy[idx, :] = projsplx(row.flatten())\n",
    "\n",
    "    return policy, Vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Regularized Softmax Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    eps = 1e-20\n",
    "    return -np.sum([xi*np.log(xi+eps) for xi in x])\n",
    "\n",
    "def discounted_entropy(policy, d):\n",
    "    state_entropies = np.apply_along_axis(entropy, 1, policy)\n",
    "    return -np.sum(d * state_entropies)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_vals = np.exp(np.float128(x))\n",
    "    return np.float64(exp_vals/np.sum(exp_vals))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V_tilde(gw, policy, d, temperature):\n",
    "    #print(\"GVT: \" + str(type(gw)))\n",
    "    val_term = gw.rho.dot(get_V(gw, policy, d))\n",
    "    ent_term = temperature * discounted_entropy(policy, d)\n",
    "    #print(val_term)\n",
    "    #print(ent_term)\n",
    "    return get_V(gw, policy, d) + temperature * discounted_entropy(policy, d)\n",
    "\n",
    "def get_Q_tilde(gw, policy, d, temperature):\n",
    "    #print(\"GQT: \" + str(type(gw)))\n",
    "    P, R = gw.P, gw.R\n",
    "    V = get_V_tilde(gw, policy, d, temperature)\n",
    "    return R + gw.gamma * np.einsum( 'ijk,k->ij', P.reshape( gw.NSTATES, gw.NACTIONS, gw.NSTATES) , V )\n",
    "\n",
    "def get_A_tilde(gw, policy, temperature, d):\n",
    "    #print(\"GAT: \" + str(type(gw)))\n",
    "    eps = 1e-20\n",
    "    t1 = get_Q_tilde(gw, policy, d, temperature)\n",
    "    t2 = temperature * np.log(policy+eps)\n",
    "    t3 = get_V_tilde(gw, policy, d, temperature)\n",
    "    #print(t1.shape)\n",
    "    #print(t2.shape)\n",
    "    #print(t3.shape)\n",
    "    ot = ((get_Q_tilde(gw, policy, d, temperature) - temperature * np.log(policy+eps)).transpose() - get_V_tilde(gw, policy, d, temperature)).transpose()\n",
    "    #print(ot.shape)\n",
    "    return ((get_Q_tilde(gw, policy, d, temperature) - temperature * np.log(policy+eps)).transpose() - get_V_tilde(gw, policy, d, temperature)).transpose()\n",
    "\n",
    "def get_grad_V_tilde(gw, policy, temperature):\n",
    "    #print(\"GGVT: \" + str(type(gw)))\n",
    "    Ptheta = get_Ptheta2D(gw.P, policy, gw)\n",
    "    d = get_d(gw.gamma, Ptheta)\n",
    "    #print((policy * get_A_tilde(gw, policy, temperature, d)).shape)\n",
    "    return ((1 / (1 - gw.gamma)) * gw.rho.dot(d).T * (policy * get_A_tilde(gw, policy, temperature, d)).T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_softmax_gradient_ascent(gw, T = 1000, eta = 0.3, temp = 0.05):\n",
    "    logits = np.ones((gw.RY, gw.RX, gw.NACTIONS))\n",
    "    #logits = np.random.rand(gw.RY, gw.RX, gw.NACTIONS)\n",
    "    logits = logits.reshape(gw.RY*gw.RX, gw.NACTIONS)\n",
    "    policy = np.apply_along_axis(softmax, 1, logits)\n",
    "    \n",
    "    #print(policy.shape)\n",
    "    #policy = policy.reshape(gw.RY*gw.RX, gw.NACTIONS)\n",
    "\n",
    "    Vs = []\n",
    "    for _ in range(T + 1):\n",
    "        ptheta = get_Ptheta2D(gw.P, policy, gw)\n",
    "        d = get_d(gw.gamma, ptheta)\n",
    "        V = get_V(gw, policy, d)\n",
    "        Vs.append(gw.rho.dot(V))\n",
    "\n",
    "        grad = get_grad_V_tilde(gw, policy, temp)\n",
    "        logits = logits + eta * grad\n",
    "        policy = np.apply_along_axis(softmax, 1, logits)\n",
    "\n",
    "    return policy, Vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Implementations\n",
    "Having implemented both algorithms, we can now move to testing them. We first begin with testing code that, given a grid world, plots how the algorithms behave over the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algorithms(gw: Gridworld, num_tests = 10, T = 1000):\n",
    "    # record variables\n",
    "    gradient_ascent_policies = []\n",
    "    gradient_ascent_values_over_learning = []\n",
    "    entropy_softmax_policies = []\n",
    "    entropy_softmax_values_over_learning = []\n",
    "\n",
    "    # do learning\n",
    "    for _ in range(num_tests):\n",
    "        policy, value = projected_gradient_ascent(gw, T)\n",
    "        gradient_ascent_policies.append(policy)\n",
    "        gradient_ascent_values_over_learning.append(value)\n",
    "        plt.plot(value, 'b')\n",
    "\n",
    "        policy, value = entropy_softmax_gradient_ascent(gw, T)\n",
    "        entropy_softmax_policies.append(policy)\n",
    "        entropy_softmax_values_over_learning.append(value)\n",
    "        plt.plot(value, 'r')\n",
    "\n",
    "    # display initial plot\n",
    "    plt.show()\n",
    "\n",
    "    # display average plot (andrew: not sure that this is actually meaningful)\n",
    "    # plt.figure()\n",
    "    # x = np.linspace(0, T + 1, T + 1)\n",
    "    # ga_mean = np.mean(gradient_ascent_values_over_learning, axis = 0)\n",
    "    # ga_std_dev = np.std(gradient_ascent_values_over_learning, axis = 0)\n",
    "    # esga_mean = np.mean(entropy_softmax_values_over_learning, axis = 0)\n",
    "    # esga_std_dev = np.std(entropy_softmax_values_over_learning, axis = 0)\n",
    "    # plt.fill_between(x, ga_mean + ga_std_dev, ga_mean - ga_std_dev, alpha=0.5, facecolor='blue')\n",
    "    # plt.fill_between(x, esga_mean + esga_std_dev, esga_mean - esga_std_dev, alpha=0.5, facecolor='red')\n",
    "    # plt.plot(ga_mean, 'b')\n",
    "    # plt.plot(esga_mean, 'r')\n",
    "    # plt.show()\n",
    "\n",
    "    return gradient_ascent_policies, entropy_softmax_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this code to compare the performance of the two algorithms, beginning with a very simple grid world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = Gridworld(6, 3, 0.5)\n",
    "gw.set_reward(4, 2, 10)\n",
    "\n",
    "ga_policies, es_policies = test_algorithms(gw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test their performance given a much larger grid world to work from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = Gridworld(10, 10, 0.5)\n",
    "gw.set_reward(5, 5, 10)\n",
    "\n",
    "ga_policies, es_policies = test_algorithms(gw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? woah, what is that graph???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing one two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = Gridworld(10, 10, 0.5)\n",
    "gw.set_reward(4, 8, 2)\n",
    "gw.set_reward(4, 2, 4)\n",
    "gw.set_reward(6, 7, 4)\n",
    "gw.set_reward(2, 2, 3)\n",
    "gw.set_reward(5, 5, 20)\n",
    "\n",
    "ga_policies, es_policies = test_algorithms(gw, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Other Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = Gridworld(6, 3, 0.5)\n",
    "gw.set_reward(4, 2, 10)\n",
    "\n",
    "for _ in range(3):\n",
    "    policy, values = entropy_softmax_gradient_ascent(gw)\n",
    "    plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = Gridworld(6, 3, 0.5)\n",
    "gw.set_reward(4, 2, 10)\n",
    "\n",
    "for _ in range(3):\n",
    "    policy, values = entropy_softmax_gradient_ascent(gw)\n",
    "    plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = Gridworld(6, 3, 0.5)\n",
    "gw.set_reward(4, 2, 10)\n",
    "\n",
    "for _ in range(2):\n",
    "    policy, values = entropy_softmax_gradient_ascent(gw)\n",
    "    plt.plot(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
